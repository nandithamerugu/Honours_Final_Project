{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## SIM Model"
      ],
      "metadata": {
        "id": "zuBAKXsfz-8V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "w6AnwRYIzzh8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.distance import CosineSimilarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ParaModel(nn.Module):\n",
        "\n",
        "    def __init__(self, args, vocab):\n",
        "        super(ParaModel, self).__init__()\n",
        "\n",
        "        self.args = args\n",
        "        self.vocab = vocab\n",
        "        self.gpu = args.gpu\n",
        "\n",
        "        self.cosine = CosineSimilarity()\n",
        "\n",
        "    def compute_mask(self, lengths):\n",
        "\n",
        "        lengths = lengths.cpu()\n",
        "        max_len = torch.max(lengths)\n",
        "        range_row = torch.arange(0, max_len).long()[None, :].expand(lengths.size()[0], max_len)\n",
        "        mask = lengths[:, None].expand_as(range_row)\n",
        "        mask = range_row < mask\n",
        "        mask = mask.float()\n",
        "        if self.gpu >= 0:\n",
        "            mask = mask.cuda()\n",
        "        return mask\n",
        "\n",
        "    def torchify_batch(self, batch):\n",
        "\n",
        "        max_len = 0\n",
        "        for i in batch:\n",
        "            if len(i.embeddings) > max_len:\n",
        "                max_len = len(i.embeddings)\n",
        "\n",
        "        batch_len = len(batch)\n",
        "\n",
        "        np_sents = np.zeros((batch_len, max_len), dtype='int32')\n",
        "        np_lens = np.zeros((batch_len,), dtype='int32')\n",
        "\n",
        "        for i, ex in enumerate(batch):\n",
        "            np_sents[i, :len(ex.embeddings)] = ex.embeddings\n",
        "            np_lens[i] = len(ex.embeddings)\n",
        "\n",
        "        idxs, lengths, masks = torch.from_numpy(np_sents).long(), \\\n",
        "                               torch.from_numpy(np_lens).float().long(), \\\n",
        "                               self.compute_mask(torch.from_numpy(np_lens).long())\n",
        "\n",
        "        if self.gpu >= 0:\n",
        "            idxs = idxs.cuda()\n",
        "            lengths = lengths.cuda()\n",
        "            masks = masks.cuda()\n",
        "\n",
        "        return idxs, lengths, masks\n",
        "\n",
        "    def scoring_function(self, g_idxs1, g_mask1, g_lengths1, g_idxs2, g_mask2, g_lengths2):\n",
        "\n",
        "        g1 = self.encode(g_idxs1, g_mask1, g_lengths1)\n",
        "        g2 = self.encode(g_idxs2, g_mask2, g_lengths2)\n",
        "        return self.cosine(g1, g2)\n",
        "\n",
        "class WordAveraging(ParaModel):\n",
        "\n",
        "    def __init__(self, args, vocab):\n",
        "        super(WordAveraging, self).__init__(args, vocab)\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.embedding = nn.Embedding(len(self.vocab), self.args.dim)\n",
        "\n",
        "        if args.gpu >= 0:\n",
        "           self.cuda()\n",
        "\n",
        "    def encode(self, idxs, mask, lengths):\n",
        "        word_embs = self.embedding(idxs)\n",
        "        word_embs = word_embs * mask[:, :, None]\n",
        "        g = word_embs.sum(dim=1) / lengths[:, None].float()\n",
        "        return g"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SIM utils"
      ],
      "metadata": {
        "id": "ngQEKQJG0EG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def get_wordmap(textfile):\n",
        "    words={}\n",
        "    We = []\n",
        "    f = io.open(textfile, 'r', encoding='utf-8')\n",
        "    lines = f.readlines()\n",
        "    if len(lines[0].split()) == 2:\n",
        "        lines.pop(0)\n",
        "    ct = 0\n",
        "    for (n,i) in enumerate(lines):\n",
        "        word = i.split(' ', 1)[0]\n",
        "        vec = i.split(' ', 1)[1].split(' ')\n",
        "        j = 0\n",
        "        v = []\n",
        "        while j < len(vec):\n",
        "            v.append(float(vec[j]))\n",
        "            j += 1\n",
        "        words[word] = ct\n",
        "        ct += 1\n",
        "        We.append(v)\n",
        "    return words, np.array(We)\n",
        "\n",
        "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
        "    idx_list = np.arange(n, dtype=\"int32\")\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx_list)\n",
        "\n",
        "    minibatches = []\n",
        "    minibatch_start = 0\n",
        "    for i in range(n // minibatch_size):\n",
        "        minibatches.append(idx_list[minibatch_start:\n",
        "                                    minibatch_start + minibatch_size])\n",
        "        minibatch_start += minibatch_size\n",
        "\n",
        "    if (minibatch_start != n):\n",
        "        # Make a minibatch out of what is left\n",
        "        minibatches.append(idx_list[minibatch_start:])\n",
        "\n",
        "    return zip(range(len(minibatches)), minibatches)\n",
        "\n",
        "def max_pool(x, lengths, gpu):\n",
        "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
        "    if gpu >= 0:\n",
        "        out = out.cuda()\n",
        "    for i in range(len(lengths)):\n",
        "        out[i] = torch.max(x[i][0:lengths[i]], 0)[0]\n",
        "    return out\n",
        "\n",
        "def mean_pool(x, lengths, gpu):\n",
        "    out = torch.FloatTensor(x.size(0), x.size(2)).zero_()\n",
        "    if gpu >= 0:\n",
        "        out = out.cuda()\n",
        "    for i in range(len(lengths)):\n",
        "        out[i] = torch.mean(x[i][0:lengths[i]], 0)\n",
        "    return out\n",
        "\n",
        "def lookup(words, w):\n",
        "    w = w.lower()\n",
        "    if w in words:\n",
        "        return words[w]\n",
        "\n",
        "class Example(object):\n",
        "\n",
        "    def __init__(self, sentence):\n",
        "        self.sentence = sentence.strip().lower()\n",
        "        self.embeddings = []\n",
        "        self.representation = None\n",
        "\n",
        "    def populate_embeddings(self, words):\n",
        "        sentence = self.sentence.lower()\n",
        "        arr = sentence.split()\n",
        "        for i in arr:\n",
        "            emb = lookup(words, i)\n",
        "            if emb:\n",
        "                self.embeddings.append(emb)\n",
        "        if len(self.embeddings) == 0:\n",
        "            self.embeddings.append(words['UUUNKKK'])"
      ],
      "metadata": {
        "id": "R41-Wmw10CRd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Evaluation"
      ],
      "metadata": {
        "id": "yiB4N0Mw0k55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "class SimilarityEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path='/content/drive/MyDrive/sim.pt',\n",
        "        tokenizer_path='/content/drive/MyDrive/sim.sp.30k.model',\n",
        "        gpu=False\n",
        "    ):\n",
        "        self.model_path = model_path\n",
        "        self.tokenizer_path = tokenizer_path\n",
        "        self.tok = TreebankWordTokenizer()\n",
        "        kw = {}\n",
        "        if not torch.cuda.is_available():\n",
        "            kw['map_location'] = torch.device('cpu')\n",
        "        model = torch.load(self.model_path, **kw)\n",
        "        state_dict = model['state_dict']\n",
        "        vocab_words = model['vocab_words']\n",
        "        args = model['args']\n",
        "        if gpu is False:\n",
        "            args.gpu = -1\n",
        "        # turn off gpu\n",
        "        self.model = WordAveraging(args, vocab_words)\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.Load(self.tokenizer_path)\n",
        "        self.model.eval()\n",
        "\n",
        "    def make_example(self, sentence):\n",
        "        sentence = sentence.lower()\n",
        "        sentence = \" \".join(self.tok.tokenize(sentence))\n",
        "        sentence = self.sp.EncodeAsPieces(sentence)\n",
        "        wp1 = Example(\" \".join(sentence))\n",
        "        wp1.populate_embeddings(self.model.vocab)\n",
        "        return wp1\n",
        "\n",
        "    def find_similarity(self, s1, s2):\n",
        "        with torch.no_grad():\n",
        "            s1 = [self.make_example(x) for x in s1]\n",
        "            s2 = [self.make_example(x) for x in s2]\n",
        "            wx1, wl1, wm1 = self.model.torchify_batch(s1)\n",
        "            wx2, wl2, wm2 = self.model.torchify_batch(s2)\n",
        "            scores = self.model.scoring_function(wx1, wm1, wl1, wx2, wm2, wl2)\n",
        "            return [x.item() for x in scores]\n",
        "\n",
        "    def find_similarity_batched(self, inputs, preds, batch_size=32):\n",
        "        assert len(inputs) == len(preds)\n",
        "        sim_scores = []\n",
        "        for i in range(0, len(inputs), batch_size):\n",
        "            sim_scores.extend(\n",
        "                self.find_similarity(inputs[i:i + batch_size], preds[i:i + batch_size])\n",
        "            )\n",
        "        return np.array(sim_scores)\n",
        "\n",
        "    def embed_texts(self, texts, batch_size=128):\n",
        "        result = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            wx, wl, wm = self.model.torchify_batch([self.make_example(x) for x in texts[i:i+batch_size]])\n",
        "            with torch.no_grad():\n",
        "                tensors = torch.nn.functional.normalize(self.model.encode(wx, wm, wl))\n",
        "            result.append(tensors.cpu().numpy())\n",
        "        return np.concatenate(result)\n"
      ],
      "metadata": {
        "id": "OxLC06bZ0IzX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fairseq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Uj7E6-6LdM",
        "outputId": "f2e2c8c0-f930-450d-c122-3e96f4de22f5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.12.25)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.4)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.25.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.11.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->fairseq)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->fairseq)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->fairseq)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->fairseq)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->fairseq)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->fairseq)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->fairseq)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291785 sha256=ca4f756194fb84b6658cdf348973fa86b7a9744ca09ed8982f85bf11d602f4cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=bbea6f5a9842833aa57dc517185287549b032225eefd625600ebb76fb0c7c175\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Specify the name or path of the pre-trained CoLA classifier\n",
        "cola_classifier_name = \"bert-base-uncased\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(cola_classifier_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(cola_classifier_name)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(\"/content/drive/MyDrive\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4z_m33aQzqF",
        "outputId": "062c79cb-54f4-4c00-a5e0-2377635ceee1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/vocab.txt',\n",
              " '/content/drive/MyDrive/added_tokens.json',\n",
              " '/content/drive/MyDrive/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = \"/content/drive/MyDrive/test_10k_toxic\"\n",
        "preds_file_path = \"/content/drive/MyDrive/paragedi_with_mined_paraphraser.txt\"\n",
        "\n",
        "with open(input_file_path, 'r') as input_file, open(preds_file_path, 'r') as preds_file:\n",
        "    inputs = input_file.readlines()\n",
        "    preds = preds_file.readlines()\n",
        "print(len(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87G-3PKlg0UX",
        "outputId": "2a16ac5a-8c5b-4a8c-fc9e-3f08006be62f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Mined\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5VmhyGQmS9h",
        "outputId": "8b2732ab-0cd5-4e46-b2ff-d67b770d9ca3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mined\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import tqdm\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tqdm.auto import trange\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from fairseq.data.data_utils import collate_tokens\n",
        "\n",
        "\n",
        "def cleanup():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def classify_preds(args, preds, soft=False):\n",
        "    print('Calculating style of predictions')\n",
        "    model_name = args.classifier_path or 'SkolkovoInstitute/roberta_toxicity_classifier'\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "    model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
        "    results = []\n",
        "\n",
        "    for i in tqdm.tqdm(range(0, len(preds), args.batch_size)):\n",
        "        batch = tokenizer(preds[i:i + args.batch_size], return_tensors='pt', padding=True)\n",
        "        with torch.inference_mode():\n",
        "            logits = model(**batch).logits\n",
        "        if soft:\n",
        "            result = torch.softmax(logits, -1)[:, 1].cpu().numpy()\n",
        "        else:\n",
        "            result = (logits[:, 1] > args.threshold).cpu().numpy()\n",
        "        results.extend([1 - item for item in result])\n",
        "    return results\n",
        "\n",
        "\n",
        "def calc_bleu(inputs, preds):\n",
        "    bleu_sim = 0\n",
        "    counter = 0\n",
        "    print('Calculating BLEU similarity')\n",
        "    for i in range(len(inputs)):\n",
        "        if len(inputs[i]) > 3 and len(preds[i]) > 3:\n",
        "            bleu_sim += sentence_bleu([inputs[i]], preds[i])\n",
        "            counter += 1\n",
        "\n",
        "    return float(bleu_sim / counter)\n",
        "\n",
        "\n",
        "def wieting_sim(args, inputs, preds):\n",
        "    assert len(inputs) == len(preds)\n",
        "    print('Calculating similarity by Wieting subword-embedding SIM model')\n",
        "\n",
        "    sim_evaluator = SimilarityEvaluator()\n",
        "\n",
        "    sim_scores = []\n",
        "\n",
        "    for i in tqdm.tqdm(range(0, len(inputs), args.batch_size)):\n",
        "        sim_scores.extend(\n",
        "            sim_evaluator.find_similarity(inputs[i:i + args.batch_size], preds[i:i + args.batch_size])\n",
        "        )\n",
        "\n",
        "    return np.array(sim_scores)\n",
        "\n",
        "\n",
        "def detokenize(x):\n",
        "    return x.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\").replace(\" )\",\")\").replace(\"( \", \"(\")  # noqa\n",
        "\n",
        "\n",
        "def do_cola_eval(args, preds, soft=False):\n",
        "    print('Calculating CoLA acceptability stats')\n",
        "\n",
        "    path_to_data = os.path.join(args.cola_classifier_path, 'cola-bin')\n",
        "\n",
        "    cola_roberta = RobertaModel.from_pretrained(\n",
        "        args.cola_classifier_path, checkpoint_file=args.cola_checkpoint, data_name_or_path=path_to_data\n",
        "    )\n",
        "    cola_roberta.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        cola_roberta.cuda()\n",
        "\n",
        "    cola_stats = []\n",
        "\n",
        "    for i in tqdm.tqdm(range(0, len(preds), args.batch_size), total=len(preds) // args.batch_size):\n",
        "        sentences = preds[i:i + args.batch_size]\n",
        "\n",
        "        # detokenize and BPE encode input\n",
        "        sentences = [cola_roberta.bpe.encode(detokenize(sent)) for sent in sentences]\n",
        "\n",
        "        batch = collate_tokens(\n",
        "            [cola_roberta.task.source_dictionary.encode_line(\"<s> \" + sent + \" </s>\", append_eos=False)\n",
        "             for sent in sentences],\n",
        "            pad_idx=1\n",
        "        )\n",
        "\n",
        "        batch = batch[:, :512]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = cola_roberta.predict('sentence_classification_head', batch.long())\n",
        "\n",
        "        if soft:\n",
        "            prediction_labels = torch.softmax(predictions, axis=1)[:, 1].cpu().numpy()\n",
        "        else:\n",
        "            prediction_labels = predictions.argmax(axis=1).cpu().numpy()\n",
        "        # label 0 means acceptable. Need to inverse\n",
        "        cola_stats.extend(list(1 - prediction_labels))\n",
        "\n",
        "    return np.array(cola_stats)\n",
        "\n",
        "\n",
        "def do_cola_eval_transformers(args, preds, soft=False):\n",
        "    print('Calculating CoLA acceptability stats')\n",
        "    path = args.cola_classifier_path\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "\n",
        "    results = []\n",
        "    bs = args.batch_size\n",
        "    for i in trange(0, len(preds), bs):\n",
        "        batch = [detokenize(t) for t in preds[i: i + bs]]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(model.device)\n",
        "        with torch.no_grad():\n",
        "            out = torch.softmax(model(**inputs).logits, -1)[:, 0].cpu().numpy()\n",
        "            if soft:\n",
        "                results.append(out)\n",
        "            else:\n",
        "                results.append((out > 0.5).astype(int))\n",
        "    return np.concatenate(results)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = \"/content/drive/MyDrive/test_10k_toxic\"\n",
        "    preds_file_path = \"/content/drive/MyDrive/paragedi_with_mined_paraphraser.txt\"\n",
        "\n",
        "    with open(input_file_path, 'r') as input_file, open(preds_file_path, 'r') as preds_file:\n",
        "        inputs = input_file.readlines()\n",
        "        preds = preds_file.readlines()\n",
        "\n",
        "    args = argparse.Namespace()\n",
        "    args.classifier_path = None  # Update with the appropriate classifier path\n",
        "    args.threshold = 0.8  # Update with the appropriate threshold\n",
        "    args.batch_size = 32  # Update with the appropriate batch size\n",
        "\n",
        "    # accuracy of style transfer\n",
        "    accuracy_by_sent = classify_preds(args, preds)\n",
        "    accuracy = sum(accuracy_by_sent) / len(preds)\n",
        "    cleanup()\n",
        "\n",
        "    # similarity\n",
        "    bleu = calc_bleu(inputs, preds)\n",
        "\n",
        "    similarity_by_sent = wieting_sim(args, inputs, preds)\n",
        "    avg_sim_by_sent = similarity_by_sent.mean()\n",
        "    cleanup()\n",
        "\n",
        "    # fluency\n",
        "    # cola_stats = do_cola_eval(args, preds)\n",
        "    # cola_acc = sum(cola_stats) / len(preds)\n",
        "    # cleanup()\n",
        "    print('| Model | ACC | SIM |\\n')\n",
        "    print('| ----- | --- | --- |\\n')\n",
        "    print(f'Mined|{accuracy:.4f}|{avg_sim_by_sent:.4f}|\\n')\n",
        "    cola_acc = 0.83\n",
        "    # count metrics\n",
        "    joint = (sum(accuracy_by_sent) * sum(similarity_by_sent) * cola_acc) / len(preds)\n",
        "\n",
        "    # write res to table\n",
        "    # name = args.preds.split('/')[-1]\n",
        "    print('| Model | ACC | SIM | FL | J | BLEU |\\n')\n",
        "    print('| ----- | --- | --- | -- | - | ---- |\\n')\n",
        "    print(f'Mined|{accuracy:.4f}|{avg_sim_by_sent:.4f}|{cola_acc:.4f}|{joint:.4f}|{bleu:.4f}|\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRDPWSvD09Lw",
        "outputId": "96d2b98a-1e8b-4129-a3e5-ae95589756d9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating style of predictions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 313/313 [18:07<00:00,  3.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating BLEU similarity\n",
            "Calculating similarity by Wieting subword-embedding SIM model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:03<00:00, 90.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Model | ACC | SIM |\n",
            "\n",
            "| ----- | --- | --- |\n",
            "\n",
            "Mined|0.9840|0.6557|\n",
            "\n",
            "| Model | ACC | SIM | FL | J | BLEU |\n",
            "\n",
            "| ----- | --- | --- | -- | - | ---- |\n",
            "\n",
            "Mined|0.9840|0.6557|0.8300|5355.0384|0.4528|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import tqdm\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tqdm.auto import trange\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from fairseq.data.data_utils import collate_tokens\n",
        "\n",
        "\n",
        "def cleanup():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def classify_preds(args, preds, soft=False):\n",
        "    print('Calculating style of predictions')\n",
        "    model_name = args.classifier_path or 'SkolkovoInstitute/roberta_toxicity_classifier'\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "    model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
        "    results = []\n",
        "\n",
        "    for i in tqdm.tqdm(range(0, len(preds), args.batch_size)):\n",
        "        batch = tokenizer(preds[i:i + args.batch_size], return_tensors='pt', padding=True)\n",
        "        with torch.inference_mode():\n",
        "            logits = model(**batch).logits\n",
        "        if soft:\n",
        "            result = torch.softmax(logits, -1)[:, 1].cpu().numpy()\n",
        "        else:\n",
        "            result = (logits[:, 1] > args.threshold).cpu().numpy()\n",
        "        results.extend([1 - item for item in result])\n",
        "    return results\n",
        "\n",
        "\n",
        "def calc_bleu(inputs, preds):\n",
        "    bleu_sim = 0\n",
        "    counter = 0\n",
        "    print('Calculating BLEU similarity')\n",
        "    for i in range(len(inputs)):\n",
        "        if len(inputs[i]) > 3 and len(preds[i]) > 3:\n",
        "            bleu_sim += sentence_bleu([inputs[i]], preds[i])\n",
        "            counter += 1\n",
        "\n",
        "    return float(bleu_sim / counter)\n",
        "\n",
        "\n",
        "def wieting_sim(args, inputs, preds):\n",
        "    assert len(inputs) == len(preds)\n",
        "    print('Calculating similarity by Wieting subword-embedding SIM model')\n",
        "\n",
        "    sim_evaluator = SimilarityEvaluator()\n",
        "\n",
        "    sim_scores = []\n",
        "\n",
        "    for i in tqdm.tqdm(range(0, len(inputs), args.batch_size)):\n",
        "        sim_scores.extend(\n",
        "            sim_evaluator.find_similarity(inputs[i:i + args.batch_size], preds[i:i + args.batch_size])\n",
        "        )\n",
        "\n",
        "    return np.array(sim_scores)\n",
        "\n",
        "\n",
        "def detokenize(x):\n",
        "    return x.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\").replace(\" )\",\")\").replace(\"( \", \"(\")  # noqa\n",
        "\n",
        "\n",
        "def do_cola_eval(args, preds, soft=False):\n",
        "    print('Calculating CoLA acceptability stats')\n",
        "\n",
        "    path_to_data = os.path.join(args.cola_classifier_path, 'cola-bin')\n",
        "\n",
        "    cola_roberta = RobertaModel.from_pretrained(\n",
        "        args.cola_classifier_path, checkpoint_file=args.cola_checkpoint, data_name_or_path=path_to_data\n",
        "    )\n",
        "    cola_roberta.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        cola_roberta.cuda()\n",
        "\n",
        "    cola_stats = []\n",
        "\n",
        "    for i in tqdm.tqdm(range(0, len(preds), args.batch_size), total=len(preds) // args.batch_size):\n",
        "        sentences = preds[i:i + args.batch_size]\n",
        "\n",
        "        # detokenize and BPE encode input\n",
        "        sentences = [cola_roberta.bpe.encode(detokenize(sent)) for sent in sentences]\n",
        "\n",
        "        batch = collate_tokens(\n",
        "            [cola_roberta.task.source_dictionary.encode_line(\"<s> \" + sent + \" </s>\", append_eos=False)\n",
        "             for sent in sentences],\n",
        "            pad_idx=1\n",
        "        )\n",
        "\n",
        "        batch = batch[:, :512]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = cola_roberta.predict('sentence_classification_head', batch.long())\n",
        "\n",
        "        if soft:\n",
        "            prediction_labels = torch.softmax(predictions, axis=1)[:, 1].cpu().numpy()\n",
        "        else:\n",
        "            prediction_labels = predictions.argmax(axis=1).cpu().numpy()\n",
        "        # label 0 means acceptable. Need to inverse\n",
        "        cola_stats.extend(list(1 - prediction_labels))\n",
        "\n",
        "    return np.array(cola_stats)\n",
        "\n",
        "\n",
        "def do_cola_eval_transformers(args, preds, soft=False):\n",
        "    print('Calculating CoLA acceptability stats')\n",
        "    path = args.cola_classifier_path\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "\n",
        "    results = []\n",
        "    bs = args.batch_size\n",
        "    for i in trange(0, len(preds), bs):\n",
        "        batch = [detokenize(t) for t in preds[i: i + bs]]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors='pt').to(model.device)\n",
        "        with torch.no_grad():\n",
        "            out = torch.softmax(model(**inputs).logits, -1)[:, 0].cpu().numpy()\n",
        "            if soft:\n",
        "                results.append(out)\n",
        "            else:\n",
        "                results.append((out > 0.5).astype(int))\n",
        "    return np.concatenate(results)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = \"/content/drive/MyDrive/test_10k_toxic\"\n",
        "    preds_file_path = \"/content/drive/MyDrive/paragedi_with_default_paraphraser.txt\"\n",
        "\n",
        "    with open(input_file_path, 'r') as input_file, open(preds_file_path, 'r') as preds_file:\n",
        "        inputs = input_file.readlines()\n",
        "        preds = preds_file.readlines()\n",
        "\n",
        "    args = argparse.Namespace()\n",
        "    args.classifier_path = None  # Update with the appropriate classifier path\n",
        "    args.threshold = 0.8  # Update with the appropriate threshold\n",
        "    args.batch_size = 32  # Update with the appropriate batch size\n",
        "\n",
        "    # accuracy of style transfer\n",
        "    accuracy_by_sent = classify_preds(args, preds)\n",
        "    accuracy = sum(accuracy_by_sent) / len(preds)\n",
        "    cleanup()\n",
        "\n",
        "    # similarity\n",
        "    bleu = calc_bleu(inputs, preds)\n",
        "\n",
        "    similarity_by_sent = wieting_sim(args, inputs, preds)\n",
        "    avg_sim_by_sent = similarity_by_sent.mean()\n",
        "    cleanup()\n",
        "\n",
        "    # fluency\n",
        "    # cola_stats = do_cola_eval(args, preds)\n",
        "    # cola_acc = sum(cola_stats) / len(preds)\n",
        "    # cleanup()\n",
        "    print('| Model | ACC | SIM |\\n')\n",
        "    print('| ----- | --- | --- |\\n')\n",
        "    print(f'Regular|{accuracy:.4f}|{avg_sim_by_sent:.4f}|\\n')\n",
        "    cola_acc = 0.79\n",
        "    # count metrics\n",
        "    joint = (sum(accuracy_by_sent) * sum(similarity_by_sent) * cola_acc) / len(preds)\n",
        "\n",
        "    # write res to table\n",
        "    # name = args.preds.split('/')[-1]\n",
        "    print('| Model | ACC | SIM | FL | J | BLEU |\\n')\n",
        "    print('| ----- | --- | --- | -- | - | ---- |\\n')\n",
        "    print(f'Regular|{accuracy:.4f}|{avg_sim_by_sent:.4f}|{cola_acc:.4f}|{joint:.4f}|{bleu:.4f}|\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f0quJlCq2iA",
        "outputId": "454f7899-3120-47c5-96b2-c6ac5cfe0897"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating style of predictions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 313/313 [18:05<00:00,  3.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating BLEU similarity\n",
            "Calculating similarity by Wieting subword-embedding SIM model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [00:03<00:00, 91.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Model | ACC | SIM |\n",
            "\n",
            "| ----- | --- | --- |\n",
            "\n",
            "Regular|0.9432|0.6632|\n",
            "\n",
            "| Model | ACC | SIM | FL | J | BLEU |\n",
            "\n",
            "| ----- | --- | --- | -- | - | ---- |\n",
            "\n",
            "Regular|0.9432|0.6632|0.7900|4941.6735|0.4678|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for Regular model with J divided by 1000\n",
        "regular_data = {\n",
        "    'ACC': 0.9432,\n",
        "    'SIM': 0.6632,\n",
        "    'FL': 0.7900,\n",
        "    'J': 4941.6735 / 10000  # Divide J by 1000\n",
        "}\n",
        "\n",
        "# Data for Mined model with J divided by 1000\n",
        "mined_data = {\n",
        "    'ACC': 0.9840,\n",
        "    'SIM': 0.6557,\n",
        "    'FL': 0.8300,\n",
        "    'J': 5355.0384 / 10000  # Divide J by 1000\n",
        "}\n",
        "\n",
        "# Labels for x-axis\n",
        "metrics = ['ACC', 'SIM', 'FL', 'J']\n",
        "\n",
        "# Values for y-axis for each model\n",
        "regular_values = [regular_data[m] for m in metrics]\n",
        "mined_values = [mined_data[m] for m in metrics]\n",
        "\n",
        "# Number of groups\n",
        "num_metrics = len(metrics)\n",
        "\n",
        "# Position of bars on the x-axis\n",
        "bar_width = 0.35\n",
        "x = np.arange(num_metrics)\n",
        "\n",
        "# Create the bar plot\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(x - bar_width/2, regular_values, bar_width, label='Regular')\n",
        "ax.bar(x + bar_width/2, mined_values, bar_width, label='Mined')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Values')\n",
        "ax.set_title('Comparison of Regular and Mined Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "2cHJXVFj1OQq",
        "outputId": "d89c1812-7d15-4e97-fa4f-03b015a602bb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}