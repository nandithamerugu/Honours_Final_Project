# -*- coding: utf-8 -*-
"""gedi_adapter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JDfmzKN95QkiBLIN3b1I8bVQomJ_v6qn
"""

import numpy as np
import os
from importlib import reload
import torch
!pip install transformers
from transformers import AutoModelForSeq2SeqLM
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer
from transformers.generation_utils import GenerationMixin

import text_processing
import config
import sys
import gc
from transformers import RobertaForSequenceClassification
from transformers import RobertaTokenizer

os.environ['CUDA_VISIBLE_DEVICES'] = '0'
sys.path.append(os.path.abspath('../transfer_utils/'))
reload(text_processing);
device = torch.device('cuda:0')
toxic_path = 'test_10k_toxic' # Path to be changed

class GediAdapter(GenerationMixin):
    def __init__(
        self,
        gedi_model,
        model,
        reg_alpha=0,
        neg_code=13086,
        pos_code=1349,
        debug=False,
        gedi_logit_coef=1,
        ub=None,
        target=0,
        lb=None,
        max_id=None,
        tokenizer=None,
        nearly_infinity = -1000,
        untouchable_tokens=None,
    ):
        self.model, self.gedi_model = model, gedi_model
        self.target, self.gedi_logit_coef = target, gedi_logit_coef
        self.POS_CODE, self.NEG_CODE = pos_code, neg_code
        self.codes = {'gedi_pos': self.POS_CODE, 'gedi_neg': self.NEG_CODE}
        self.logits = []
        self.tokenizer = tokenizer
        self.untouchable_tokens, self.nearly_infinity = untouchable_tokens or [], nearly_infinity
        self.max_id = max_id
        self.debug = debug
        self.reg_alpha = reg_alpha
        self.ub , self.lb = ub, lb

    def return_output(self, return_dict=True, **new_args):
      with torch.no_grad():
        outputs = self.model(return_dict=return_dict, **new_args)
      return outputs

    def __call__(self, return_dict=True, **kwargs):
        new_args = kwargs.get('main', {})

        outputs = self.return_output(True, **new_args)
        outputs['main'] = outputs

        gedi_logits = {}

        for _,gedi_key in enumerate(['gedi_pos', 'gedi_neg']):
            gedi_args = kwargs.get(gedi_key, {})
            with torch.no_grad():
                gedi_out = self.gedi_model(**gedi_args, return_dict=True)
            outputs[gedi_key] = gedi_out
            gedi_logits[gedi_key] = gedi_out['logits'][:,-1]
        stacked = torch.stack([gedi_logits['gedi_pos'], gedi_logits['gedi_neg']])
        for _,token_id in enumerate(self.untouchable_tokens):
            stacked[:, :, token_id] = self.nearly_infinity

        old_logits = torch.log_softmax(stacked, -1) if not(self.reg_alpha) else torch.log(torch.softmax(stacked, -1) + self.reg_alpha)
        old_logits =old_logits+ self.gedi_model.logit_scale if (hasattr(self.gedi_model, 'logit_scale') == True) else 0
        old_logits =old_logits+ self.gedi_model.bias.reshape(2, 1, 1).repeat(1,1,old_logits.shape[-1]) if hasattr(self.gedi_model, 'bias') == True else 0

        sm, logits = torch.log_softmax(old_logits, 0) , outputs['logits'][:,-1]
        shift =  sm[self.target]
        shift = shift- shift.mean()
        if not(self.lb is None and self.ub is None):
            shift = torch.clamp(shift, self.lb, self.ub) if not(self.lb is None and self.ub is None) else 0

        for  _,token_id in enumerate(self.untouchable_tokens):
            shift[:, token_id] = 0

        shift_add = shift * self.gedi_logit_coef
        corrected = logits + shift_add

        # corr_info = self.show_correction(sm, logits, corrected, torch.log_softmax(stacked, -1)) if self.debug else 0
        # if not(self.max_id is None):
        corrected[self.max_id:] = -np.infty if not(self.max_id is None) else corrected[self.max_id:]
        outputs['logits'] = corrected.unsqueeze(1)
        return outputs

    def prepare_inputs_for_generation(self, input_ids, **kwargs):
        past, result = kwargs.get('past'), {}

        if not(not past or isinstance(past, tuple)):
            for key, val in past.items():
                kwargs[key]['past'] = val

        main_kwargs, main_input_ids = kwargs.get('main', kwargs), input_ids

        if not(not main_kwargs.get('past') == None or not kwargs.get('main_prefix') != None):
            prefix = kwargs['main_prefix']
            prefix = prefix.unsqueeze(0)
            prefix = prefix.repeat(main_input_ids.shape[0], 1)

            main_input_ids = torch.cat([prefix, main_input_ids], dim=1)

            if not(main_kwargs.get('attention_mask') == None):
                main_kwargs['attention_mask'] = torch.cat([prefix * 0 + 1, main_kwargs['attention_mask']], dim=1)
        result['main'] = self.model.prepare_inputs_for_generation(main_input_ids, **main_kwargs)

        for _, k in enumerate(['gedi_pos', 'gedi_neg']):
            gedi_args = kwargs.get(k, {})
            if not(kwargs.get('gedi_prepend')):
              new_input_ids = input_ids.clone()
              new_input_ids[:, 0] = self.codes[k]
            else:
              prefix = torch.ones([input_ids.shape[0], 1], dtype=input_ids.dtype)
              prefix = prefix.to(input_ids.device) * self.codes[k]
              new_input_ids = torch.cat([prefix, input_ids], dim=1)

            result[k] = self.gedi_model.prepare_inputs_for_generation(new_input_ids, **gedi_args)
        return result

    def paraphrase(self, text, n=None, max_length=128):
        inputs = tokenizer(text, return_tensors='pt', padding=True)['input_ids']
        inputs = inputs.to(device)

        if not(max_length != 'auto'):
          if int(inputs.shape[1] * 1.1) + 4 > 64:
            max_length = 64
          else:
            max_length = int(inputs.shape[1] * 1.1) + 4

        result = adapter.generate(
            inputs,
            repetition_penalty=3.0,
            num_return_sequences=1 or n,
            max_length=max_length,
            bad_words_ids=[[2]],  # unk
            temperature=0.0,
            do_sample=False,

        )
        texts = []
        for _, r in result:
          texts.append(tokenizer.decode(r))

        if n:
          return texts
        else:
            return texts[0]

    def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder=False):
        result = {}
        for key, val in model_kwargs.items():
          result[key] = val

        result['main'] = self.model._update_model_kwargs_for_generation(
            model_kwargs=model_kwargs.get('main', model_kwargs),
            outputs=outputs['main'],
            is_encoder_decoder=self.model.config.is_encoder_decoder,
        )
        for _, k in enumerate(['gedi_pos', 'gedi_neg']):
            t_model_kwargs, t_outputs, t_is_encoder_decoder = model_kwargs.get(k, {}), outputs[k], self.gedi_model.config.is_encoder_decoder
            result[k] = self.gedi_model._update_model_kwargs_for_generation(
                model_kwargs = t_model_kwargs,
                outputs = t_outputs,
                is_encoder_decoder = t_is_encoder_decoder,
            )

        result['past'] = {
            k: result[k]['past']
            for _, k in enumerate(['main', 'gedi_pos', 'gedi_neg'])
            if 'past' in result[k]
              if result[k]['past'] != None
                if result[k]['past'][0] != None
        }
        return result

    def get_output_embeddings(self):
        return True

    def _reorder_cache(self, past, beam_idx):
        result = {}
        # prev_ =
        for key, subpast in past.items():
          if key != 'main':
            model = self.gedi_model
            result[key] = model._reorder_cache(subpast, beam_idx)
          else:
            model = self.model
            result[key] = model._reorder_cache(subpast, beam_idx)

        return result

    config = property(lambda self: self.model.config)
    get_encoder = lambda self: self.model.get_encoder()
    parameters = lambda self: self.model.parameters()

    device = property(lambda self: self.model.device)

    main_input_name = property(lambda self: self.model.main_input_name)
    def forward(self, attention_mask=None, **kwargs):
        pass

tokenizer = AutoTokenizer.from_pretrained(para_model_name)

para = AutoModelForSeq2SeqLM.from_pretrained(para_model_name)
len_tokenizer = len(tokenizer)
para.resize_token_embeddings(len_tokenizer)

gedi_dis = AutoModelForCausalLM.from_pretrained(gedi_model_name)

def positiveAndNegativeEncodings(str):
  return tokenizer.encode(str, add_special_tokens=False)[0]

def cleanup():
    gc.collect()
    if torch.cuda.is_available() == True:
      if not(device.type == 'cpu') == True:
          with torch.cuda.device(device):
              torch.cuda.empty_cache()

def addToDevice():
  para.to(device);
  gedi_dis.to(device);
  gedi_dis.bias, gedi_dis.logit_scale = gedi_dis.bias.to(device), gedi_dis.logit_scale.to(device)

def allEval():
  para.eval();
  gedi_dis.eval();

gedi_dis.bias, gedi_dis.logit_scale = torch.tensor([[ 0.08441592, -0.08441573]]), torch.tensor([[1.2701858]])
print(gedi_dis.bias, gedi_dis.logit_scale)

addToDevice()
allEval()

with open(toxic_path, 'r') as f:
    test_data = []
    for line in f.readlines():
      test_data.append(line.strip())

def paraphrase1(text, n=None, max_length='auto', beams=2):

    if isinstance(text, str):
      texts = [text]
    else:
      texts = text
    temp_text = []
    for t in texts:
      temp_text.append(text_processing.text_preprocess(t))

    texts = temp_text
    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids']
    inputs = inputs.to(dadapter.device)
    if max_length == 'auto':
        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)

    result = dadapter.generate(
        inputs,
        temperature=0.0,
        num_return_sequences= not(n and 1),
        repetition_penalty=3.0,
        do_sample=False,
        max_length=max_length,
        num_beams=beams,
        bad_words_ids=[[2]] # unk
    )

    temp_text = []
    for r in result:
      temp_text_append = tokenizer.decode(r, skip_special_tokens=True)
      temp_text.append(temp_text_append)

    texts = temp_text
    temp_text = []
    for t in texts:
      temp_text_append = text_processing.text_postprocess(t)
      temp_text.append(temp_text_append)
    texts = temp_text
    if not n:
      if isinstance(text, str):
        return texts[0]
    return texts

reg_alpha, ub = 3e-5, 0.01
dadapter = GediAdapter(
    gedi_logit_coef=10,
    gedi_model=gedi_dis,
    model=para,
    tokenizer=tokenizer,
    neg_code=positiveAndNegativeEncodings('toxic'),
    pos_code=positiveAndNegativeEncodings('normal'),
    target=0,
    ub=ub,
    reg_alpha=reg_alpha
)

paraphrase1(test_data[:3])

"""# Evaluate"""

clf, clf_tokenizer = RobertaForSequenceClassification.from_pretrained(classifier_model_name).to(device), RobertaTokenizer.from_pretrained(classifier_model_name)

def predict_toxicity(texts):
    with torch.inference_mode():
        inputs = clf_tokenizer(texts, return_tensors='pt', padding=True)
        inputs = inputs.to(clf.device)
        temp_out = torch.softmax(clf(**inputs).logits, -1)[:, 1]
        temp_out = temp_out.cpu()
        temp_out = temp_out.numpy()
        out = temp_out
    return out

predict_toxicity(['hello life', 'I will fuck you', 'Hi fucking bitch', 'bastard', 'motherfucker', 'desperate'])

"""# The baseline"""

def paraphrase2(text, max_length='auto', beams=5, rerank=True):
    if isinstance(text, str):
      texts = [text]
    else:
      texts = text
    temp_text = []
    for t in texts:
      temp_text.append(text_processing.text_preprocess(t))

    texts = temp_text
    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids']
    inputs = inputs.to(adapter2.device)
    if max_length == 'auto':
        max_length = min(int(inputs.shape[1] * 1.1) + 4, 64)
    attempts = beams
    out = adapter2.generate(
        inputs,
        bad_words_ids=[[2]],  # unk
        temperature=1.0,
        num_beams=beams,
        max_length=max_length,
        do_sample=False,
        return_dict_in_generate=True,
        repetition_penalty=3.0,
        output_scores=True,
        num_return_sequences=attempts,
    )

    temp_results = []
    for r in out.sequences:
      temp_results.append(tokenizer.decode(r, skip_special_tokens=True))
    results = temp_results

    if rerank == True:
        scores = predict_toxicity(results)

    temp_results = []
    for t in results:
      temp_results.append(text_processing.text_postprocess(t))
    results = temp_results

    out_texts = []
    for i in range(len(texts)):
        if rerank == False:
            idx = 0
        else:
            temp_scores = scores[(i*attempts):((i+1)*attempts)]
            idx = temp_scores.argmin()
        out_texts.append(results[i*attempts+idx])
    return out_texts

adapter2 = GediAdapter(
    model=para,
    tokenizer=tokenizer,
    gedi_model=gedi_dis,
    gedi_logit_coef=10,
    target=0,
    untouchable_tokens=[0, 1],
    neg_code=positiveAndNegativeEncodings('toxic'),
    pos_code=positiveAndNegativeEncodings('normal'),
    reg_alpha=reg_alpha,
    ub=ub
)

torch.manual_seed(0)

beams_rerank = [[3, True],
                [3, False],
                [10, False],
                [10, True]]

eval_text = ['fuck you!', 'you are stupid!', 'you remind me of the chump .', 'he has to be a terrorist ! .']
for i in beams_rerank:
  print(paraphrase2(eval_text, beams=i[0], rerank=i[1]))

batch_size = 2

import os
from tqdm.auto import tqdm, trange
def checkBatch(batch):
  if not batch:
    return True
  else:
    return False
cleanup()

lines = test_data[:10]


for i in trange(int(len(lines) / batch_size + 1)):
    if i % 10 == 0:
        cleanup()
    t = i * batch_size
    batch = []
    for line in lines[t:(t+batch_size)]:
      batch.append(line.strip())
    if checkBatch(batch):
        continue

    res = paraphrase2(batch, max_length='auto', beams=10)

    for out in res:
        print(out)

"""Expected output:

```
You'd be right. You'd be right. You'd
As snooty and overbearing as its
A bad society does the wrong things, and votes for the wrong politicians.
President, he's the man. He's the man.
Colberg's a bad guy, and I'm a TSA.
Dubious, dubious, dubious, dubious.
I like you. I think you'll be an intelligent man and your contributions will be a big deal.
Luna's oh-so-stunnel, immature girl......who has no idea how to do it, which smells bad, I'd like to see
Mormonis is the wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The wrong thing to do. The right thing to do. The right thing to do. The right thing to do
You'd be a bad guy, uninitiated.
```
"""